{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user_data/sbharad2/espnet/tools/miniconda/envs/spclip/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "base_module_pth = \"/data/user_data/sbharad2/SpeechCLIP\"\n",
    "sys.path.append(os.path.abspath(base_module_pth))\n",
    "\n",
    "import argparse\n",
    "import warnings\n",
    "import importlib\n",
    "\n",
    "from avssl import task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--task\", default=\"TrainKWClip_GeneralTransformer\", type=str, required=False\n",
    ")\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "runner = getattr(task, args.task)()\n",
    "parser = runner.add_args(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldsysargv = sys.argv\n",
    "cmdstring = \"TrainKWClip_GeneralTransformer --name=flickr_analysis --text_file=Flickr8k.flickr_train_sampled.token.txt --resume=/data/user_data/sbharad2/SpeechCLIP/slt_ckpts/SpeechCLIP/base/flickr/parallel/epoch_131-step_15443-val_recall_mean_1_36.0100.ckpt --dataset_root=data/flickr --gpus=1 --njobs=4 --seed=7122 --test --save_path=exp_test\"\n",
    "sys.argv = cmdstring.split()\n",
    "args = runner.parse_args(parser)\n",
    "# sys.argv = oldsysargv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 7122\n",
      "/data/user_data/sbharad2/espnet/tools/miniconda/envs/spclip/lib/python3.8/site-packages/pytorch_lightning/utilities/cloud_io.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=map_location)\n",
      "/data/user_data/sbharad2/espnet/tools/miniconda/envs/spclip/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:2054: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
      "  from torch.distributed._sharded_tensor import pre_load_state_dict_hook, state_dict_hook\n",
      "/data/user_data/sbharad2/espnet/tools/miniconda/envs/spclip/lib/python3.8/site-packages/fairseq/checkpoint_utils.py:315: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(f, map_location=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/sbharad2/.cache/torch/hub/s3prl_cache/5e6b91abd59b390dc3f89225f0e7d26f5bcb6fac496a08110c8862e3b1bb93e7\n",
      "for https://dl.fbaipublicfiles.com/hubert/hubert_base_ls960.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 19:32:03 | INFO | fairseq.tasks.hubert_pretraining | current directory is /data/user_data/sbharad2/SpeechCLIP\n",
      "2024-11-08 19:32:03 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/wnhsu/data/librispeech/960h/iter/250K_50hz_km100_mp0_65_v2', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2024-11-08 19:32:03 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}\n",
      "/data/user_data/sbharad2/espnet/tools/miniconda/envs/spclip/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "2024-11-08 19:32:04 | INFO | avssl.module.speech_encoder_plus | Normalize waveform = (False)\n",
      "2024-11-08 19:32:04 | INFO | avssl.module.speech_encoder_plus | Loaded s3prl speech encoder (hubert): out_dim = 768 layer_drop = 0.0\n",
      "2024-11-08 19:32:04 | INFO | avssl.module.speech_encoder_plus | Using weighted sum for all hiddenstates(13)\n",
      "2024-11-08 19:32:06 | WARNING | avssl.module.clip_official | Reduce text embedding to size of 8112\n",
      "2024-11-08 19:32:06 | INFO | avssl.model.kwClip | Create Parallel Branch\n",
      "2024-11-08 19:32:06 | INFO | avssl.model.kwClip | Using TransformerEncoder as KW_ParallelBranch (projection=True)\n",
      "2024-11-08 19:32:06 | INFO | avssl.module.kw_modules.TransformerModels | Using 1 layer transformer encoder\n",
      "2024-11-08 19:32:06 | INFO | avssl.model.kwClip | Start init [CLS] torch.Size([1, 1, 768])\n",
      "2024-11-08 19:32:07 | INFO | avssl.data.flickr_dataset | Load clip (ViT-B/32) for image transform\n",
      "2024-11-08 19:32:09 | INFO | avssl.data.flickr_dataset | Flickr8k (flickr_train_sampled): 1978 samples and skipped 7912\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "/data/user_data/sbharad2/espnet/tools/miniconda/envs/spclip/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/native_amp.py:53: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/data/user_data/sbharad2/espnet/tools/miniconda/envs/spclip/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:45: LightningDeprecationWarning: Setting `Trainer(resume_from_checkpoint=)` is deprecated in v1.5 and will be removed in v1.7. Please pass `Trainer.fit(ckpt_path=)` directly instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "Restoring states from the checkpoint path at /data/user_data/sbharad2/SpeechCLIP/slt_ckpts/SpeechCLIP/base/flickr/parallel/epoch_131-step_15443-val_recall_mean_1_36.0100.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for images from data/flickr/Flickr_8k.flickr_train_sampledImages.txt\n",
      "Loaded captions for 2000 images from data/flickr/Flickr8k.flickr_train_sampled.token.txt\n",
      "Missing  1155138244_859fd6e079_0.wav\n",
      "Missing  1468103286_96a6e07029_0.wav\n",
      "Missing  1479857177_9d4a6f38fd_0.wav\n",
      "Missing  1643915227_9f48068772_0.wav\n",
      "Missing  1797554350_20998753c0_1.wav\n",
      "Missing  1808504612_3508f3c9bb_0.wav\n",
      "Missing  199463720_329a802206_0.wav\n",
      "Missing  2058091220_2087270068_0.wav\n",
      "Missing  2087317114_cf06df5aa5_1.wav\n",
      "Missing  2136455112_202c093ba4_0.wav\n",
      "Missing  2221818690_9003756d33_1.wav\n",
      "Missing  2258277193_586949ec62.jpg.1#2.wav\n",
      "Missing  2319197581_94f807b204_0.wav\n",
      "Missing  236095031_5cb17dc54a_0.wav\n",
      "Missing  2394824046_51cec8e5e7_1.wav\n",
      "Missing  240696675_7d05193aa0_0.wav\n",
      "Missing  2410153942_ba4a136358_0.wav\n",
      "Missing  2428275562_4bde2bc5ea_1.wav\n",
      "Missing  2553619107_d382a820f9_1.wav\n",
      "Missing  2557972410_6925fe695c_2.wav\n",
      "Missing  2582390123_71120edb0c_1.wav\n",
      "Missing  2616508003_fa5ca5780d_0.wav\n",
      "Loaded 2000 caption file names\n",
      "First 5: ['2149982207_5345633bbf_0.wav', '141139674_246c0f90a1_0.wav', '2085726719_a57a75dbe5_0.wav', '1288909046_d2b2b62607_0.wav', '241347580_a1e20321d3_0.wav']\n",
      "Skiping because there is no audio for 1155138244_859fd6e079.\n",
      "Skiping because there is no audio for 1468103286_96a6e07029.\n",
      "Skiping because there is no audio for 1479857177_9d4a6f38fd.\n",
      "Skiping because there is no audio for 1643915227_9f48068772.\n",
      "Skiping because there is no audio for 1797554350_20998753c0.\n",
      "Skiping because there is no audio for 1808504612_3508f3c9bb.\n",
      "Skiping because there is no audio for 199463720_329a802206.\n",
      "Skiping because there is no audio for 2058091220_2087270068.\n",
      "Skiping because there is no audio for 2087317114_cf06df5aa5.\n",
      "Skiping because there is no audio for 2136455112_202c093ba4.\n",
      "Skiping because there is no audio for 2221818690_9003756d33.\n",
      "Skiping because there is no audio for 2258277193_586949ec62.\n",
      "Skiping because there is no audio for 2319197581_94f807b204.\n",
      "Skiping because there is no audio for 236095031_5cb17dc54a.\n",
      "Skiping because there is no audio for 2394824046_51cec8e5e7.\n",
      "Skiping because there is no audio for 240696675_7d05193aa0.\n",
      "Skiping because there is no audio for 2410153942_ba4a136358.\n",
      "Skiping because there is no audio for 2428275562_4bde2bc5ea.\n",
      "Skiping because there is no audio for 2553619107_d382a820f9.\n",
      "Skiping because there is no audio for 2557972410_6925fe695c.\n",
      "Skiping because there is no audio for 2582390123_71120edb0c.\n",
      "Skiping because there is no audio for 2616508003_fa5ca5780d.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user_data/sbharad2/espnet/tools/miniconda/envs/spclip/lib/python3.8/site-packages/pytorch_lightning/utilities/cloud_io.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=map_location)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at /data/user_data/sbharad2/SpeechCLIP/slt_ckpts/SpeechCLIP/base/flickr/parallel/epoch_131-step_15443-val_recall_mean_1_36.0100.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating:   0%|          | 1/248 [00:03<15:24,  3.74s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user_data/sbharad2/espnet/tools/miniconda/envs/spclip/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:59: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 248/248 [02:13<00:00, 20.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user_data/sbharad2/espnet/tools/miniconda/envs/spclip/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:59: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total #1978 images, #1978 audio\n",
      "val_recall_AI {'recall@1': 59.605664014816284, 'recall@5': 84.68149900436401, 'recall@10': 92.46714115142822}\n",
      "val_recall_IA {'recall@1': 66.27907156944275, 'recall@5': 88.7259840965271, 'recall@10': 94.13549304008484}\n",
      "val_recall_mean {'recall@1': 62.94236779212952, 'recall@5': 86.70374155044556, 'recall@10': 93.30131709575653}\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 VALIDATE RESULTS\n",
      "{'val_cl_temp': 14.285698890686035,\n",
      " 'val_cl_temp_epoch': 14.285698890686035,\n",
      " 'val_loss': 0.18520741164684296,\n",
      " 'val_loss_epoch': 0.18520741164684296,\n",
      " 'val_p_cl_loss': 0.18520741164684296,\n",
      " 'val_p_cl_loss_epoch': 0.18520741164684296}\n",
      "--------------------------------------------------------------------------------\n",
      "Validating: 100%|██████████| 248/248 [02:13<00:00,  1.86it/s]\n"
     ]
    }
   ],
   "source": [
    "runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
