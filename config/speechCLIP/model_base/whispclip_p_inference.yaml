data:
  dataset:
    name: flickr
    dataset_root: data/flickr
    text_file: Flickr8k.token.txt
    clip_image_transform: ViT-B/32
    load_image: true
    load_audio: true
    tokenizeText: true
    modalities: [audio, text]
  batch_size: 256 # 768 # 256
  dev_batch_size: 128
  split_ratio: 0.9

model_settings:
  cascaded_objective_weight: 0.0
  parallel_objective_weight: 1.0
  cosine_objective_weight: 1.0 # This is image embedding space regularization weight
  initialize_weights: false # xavier and normal give nan
  image_stats_path:  data/flickr_stats/ViT_B_32.npy
  text_stats_path: data/flickr_stats/text_stats.CLIP_ViT_B_32.npy
  speech_stats_path: data/flickr_stats/speech_stats.contrastive_e208.npy
  parallel_branch:
    # name: integrate # [ "integrate", "individual" ]
    # integrate: add another [CLS] token for parallel objective
    # individual: add another [CLS] token and another branch transformer encoder for parallel objective
    transformer_type: TransformerEncoder
    transformer_args:
        n_layers: 1
        d_model: 768
        nhead: 8
        dim_feedforward: 3072
        dropout: 0.1
        activation: gelu
        layer_norm_eps: 1.0e-5
        batch_first: True
        norm_first: False
    need_projection: true
  
  cascaded_branch:
    type: KW_CascadedBranch #KW_CascadedBranch #KW_CascadedBranch_Integrated
    transformer_type: MultiheadAttentionAndNorm #[TransformerEncoder, MultiheadAttentionAndNorm, RelTransformerEncoder]
    transformer_args:
        n_layers: 1
        d_model: 768
        nhead: 1
        dim_feedforward: 3072
        dropout: 0.1
        activation: gelu
        layer_norm_eps: 1.0e-5
        batch_first: True
        norm_first: False
    keyword:
        number: 8
        detokenized_K_neighbors: 5
        retrieve_method: cosine # cosine or pseudo_inverse
        batchnorms:
            type: eachKw #eachKw or same
            std_scale: 1.0
            learnable: true
            parallel: true
        attention_constraints:
            diversity_per_kw_loss_weight : 0.0
            diversity_per_frame_loss_weight : 0.0
            smoothness_per_frame_loss_weight : 0.0
    vq:
        bn_before_vq : true
        activation: gelu # relu or gelu
        type: SimpleVectorQuantizer # which type of quantizer to use, gumbel or kmeans
        args:
            temp: fixed=0.1 #[2.0, 0.5, 0.999995]
            time_first: true
            use_gumbel: false
            hard: true
  
cl_loss:
  type: MaskedContrastiveLoss # SupConLoss
  args:
    temperature: 0.07
    temperature_trainable: False
    margin: 0.0
    dcl: false
    a2b:  true
    b2a: true

retrieval:
  audio_feat_src: parallel  # ["parallel","cascaded"]
  recall_at: [1,5,10]


clip:
  name: ViT-B/32
  image_encoder_trainable: false
  text_encoder_trainable: false
  reduce_subword_embbedding: ./avssl/data/flickr_stat/text_clip_vocab_usage_byfreq.npy  # npy file for the selected embedding ID and its frequency


audio_encoder:
  type: whisper
  model_name: tiny # tiny for debugging
  trainable: false # only train the projection layer if false, oterwise train the whole model
  optim:
    name: Adam
    args:
      lr: 1.e-5 # 1.e-4
      weight_decay: 1.e-6
  scheduler:
    name: linear_warmup_decay
    warmup: 5000 # 1700 # 5000
    max_step: 50000 # 17000 # 50000
    final_lr: 1.e-8


trainer:
  max_steps: 50000 # 17000 # 50000
  gradient_clip_val: 2 # 4
  track_grad_norm: 2
  accumulate_grad_batches: 1
  check_val_every_n_epoch: 1
  precision: 16
  logger: wandb
  log_every_n_steps: 8
  default_root_dir: exp/wshiper_clip_base_p_flickr
  num_sanity_val_steps: 0
  strategy: dp
  # limit_val_batches: 16 # small val for fast train
  # limit_train_batches: 8 # debugging

log_setting:
  log_detokenize_results: true # whether or not to save the results of detokenized vq output
  log_detokenize_results_every_n_epoch: 5
  log_draw_pca_every_n_epoch: 10

logger:
  project: TransitiveBind
