{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "base_module_pth = \"/data/user_data/sbharad2/SpeechCLIP\"\n",
    "sys.path.append(os.path.abspath(base_module_pth))\n",
    "\n",
    "import argparse\n",
    "import warnings\n",
    "import importlib\n",
    "\n",
    "from avssl import task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--task\", default=\"TrainKWClip_GeneralTransformer\", type=str, required=False\n",
    ")\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "runner = getattr(task, args.task)()\n",
    "parser = runner.add_args(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldsysargv = sys.argv\n",
    "cmdstring = \"TrainKWClip_GeneralTransformer --resume=/data/user_data/sbharad2/SpeechCLIP/slt_ckpts/SpeechCLIP/base/flickr/parallel/epoch_131-step_15443-val_recall_mean_1_36.0100.ckpt --dataset_root=data/flickr --gpus=1 --njobs=4 --seed=7122 --test --save_path=exp_test\"\n",
    "sys.argv = cmdstring.split()\n",
    "args = runner.parse_args(parser)\n",
    "# sys.argv = oldsysargv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 7122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/sbharad2/.cache/torch/hub/s3prl_cache/5e6b91abd59b390dc3f89225f0e7d26f5bcb6fac496a08110c8862e3b1bb93e7\n",
      "for https://dl.fbaipublicfiles.com/hubert/hubert_base_ls960.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 19:46:24 | INFO | fairseq.tasks.hubert_pretraining | current directory is /data/user_data/sbharad2/SpeechCLIP\n",
      "2024-11-06 19:46:24 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/wnhsu/data/librispeech/960h/iter/250K_50hz_km100_mp0_65_v2', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2024-11-06 19:46:24 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}\n",
      "2024-11-06 19:46:25 | INFO | avssl.module.speech_encoder_plus | Normalize waveform = (False)\n",
      "2024-11-06 19:46:25 | INFO | avssl.module.speech_encoder_plus | Loaded s3prl speech encoder (hubert): out_dim = 768 layer_drop = 0.0\n",
      "2024-11-06 19:46:25 | INFO | avssl.module.speech_encoder_plus | Using weighted sum for all hiddenstates(13)\n",
      "2024-11-06 19:46:28 | WARNING | avssl.module.clip_official | Reduce text embedding to size of 8112\n",
      "2024-11-06 19:46:28 | INFO | avssl.model.kwClip | Create Parallel Branch\n",
      "2024-11-06 19:46:28 | INFO | avssl.model.kwClip | Using TransformerEncoder as KW_ParallelBranch (projection=True)\n",
      "2024-11-06 19:46:28 | INFO | avssl.module.kw_modules.TransformerModels | Using 1 layer transformer encoder\n",
      "2024-11-06 19:46:28 | INFO | avssl.model.kwClip | Start init [CLS] torch.Size([1, 1, 768])\n",
      "2024-11-06 19:46:28 | INFO | avssl.data.flickr_dataset | Load clip (ViT-B/32) for image transform\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "OrderedNamespace({'task': 'TrainKWClip_GeneralTransformer', 'config': '', 'save_path': 'exp_test', 'train': False, 'eval': False, 'test': True, 'ckpt': '/data/user_data/sbharad2/SpeechCLIP/slt_ckpts/SpeechCLIP/base/flickr/parallel/epoch_131-step_15443-val_recall_mean_1_36.0100.ckpt', 'resume': '/data/user_data/sbharad2/SpeechCLIP/slt_ckpts/SpeechCLIP/base/flickr/parallel/epoch_131-step_15443-val_recall_mean_1_36.0100.ckpt', 'njobs': 4, 'gpus': 1, 'seed': 7122, 'log_level': 'info', 'data': {'dataset': {'name': 'flickr', 'dataset_root': 'data/flickr', 'text_file': 'Flickr8k.token.txt', 'clip_image_transform': 'ViT-B/32', 'load_image': True, 'load_audio': True, 'tokenizeText': True, 'modalities': ['audio', 'image', 'text']}, 'batch_size': 256, 'dev_batch_size': 8, 'split_ratio': 0.9}, 'model_settings': {'cascaded_objective_weight': 0, 'parallel_objective_weight': 1.0, 'parallel_branch': {'transformer_type': 'TransformerEncoder', 'transformer_args': {'n_layers': 1, 'd_model': 768, 'nhead': 8, 'dim_feedforward': 3072, 'dropout': 0.1, 'activation': 'gelu', 'layer_norm_eps': 1e-05, 'batch_first': True, 'norm_first': False}}, 'cascaded_branch': {'type': 'KW_CascadedBranch', 'transformer_type': 'MultiheadAttentionAndNorm', 'transformer_args': {'n_layers': 1, 'd_model': 768, 'nhead': 1, 'dim_feedforward': 3072, 'dropout': 0.1, 'activation': 'gelu', 'layer_norm_eps': 1e-05, 'batch_first': True, 'norm_first': False}, 'keyword': {'number': 8, 'detokenized_K_neighbors': 5, 'retrieve_method': 'cosine', 'batchnorms': {'type': 'eachKw', 'std_scale': 1.0, 'learnable': True, 'parallel': True}, 'attention_constraints': {'diversity_per_kw_loss_weight': 0.0, 'diversity_per_frame_loss_weight': 0.0, 'smoothness_per_frame_loss_weight': 0.0}}, 'vq': {'bn_before_vq': True, 'activation': 'gelu', 'type': 'SimpleVectorQuantizer', 'args': {'temp': 'fixed=0.1', 'time_first': True, 'use_gumbel': False, 'hard': True}}}}, 'cl_loss': {'type': 'MaskedContrastiveLoss', 'args': {'temperature': 0.07, 'temperature_trainable': False, 'margin': 0.0, 'dcl': False, 'a2b': True, 'b2a': True}}, 'retrieval': {'audio_feat_src': 'parallel', 'recall_at': [1, 5, 10]}, 'clip': {'name': 'ViT-B/32', 'image_encoder_trainable': False, 'text_encoder_trainable': False, 'reduce_subword_embbedding': './avssl/data/flickr_stat/text_clip_vocab_usage_byfreq.npy'}, 'audio_encoder': {'type': 'FairseqHubert', 'name': 'hubert', 'pretrained': True, 'trainable': False, 'feat_select_idx': 'weighted_sum', 'layer_drop': 0.0, 'max_audio_len': 102400, 'optim': {'name': 'Adam', 'args': {'lr': 0.0001, 'weight_decay': 1e-06}}, 'scheduler': {'name': 'linear_warmup_decay', 'warmup': 5000, 'max_step': 50000, 'final_lr': 1e-08}}, 'trainer': {'max_steps': 50000, 'gradient_clip_val': 4, 'accumulate_grad_batches': 1, 'check_val_every_n_epoch': 1, 'precision': 16, 'logger': <pytorch_lightning.loggers.wandb.WandbLogger object at 0x7f3086d2e880>, 'log_every_n_steps': 8, 'default_root_dir': 'exp/KW_bsz256_WS_p1_flickr', 'num_sanity_val_steps': 0, 'strategy': 'dp'}, 'log_setting': {'log_detokenize_results': True, 'log_detokenize_results_every_n_epoch': 20, 'log_draw_pca_every_n_epoch': 50}, 'logger': {'project': 'kw-general'}})\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 19:46:30 | INFO | avssl.data.flickr_dataset | Flickr8k (test): 5000 samples\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "/data/user_data/sbharad2/espnet/tools/miniconda/envs/spclip/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/native_amp.py:53: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/data/user_data/sbharad2/espnet/tools/miniconda/envs/spclip/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:45: LightningDeprecationWarning: Setting `Trainer(resume_from_checkpoint=)` is deprecated in v1.5 and will be removed in v1.7. Please pass `Trainer.fit(ckpt_path=)` directly instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "Restoring states from the checkpoint path at /data/user_data/sbharad2/SpeechCLIP/slt_ckpts/SpeechCLIP/base/flickr/parallel/epoch_131-step_15443-val_recall_mean_1_36.0100.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at /data/user_data/sbharad2/SpeechCLIP/slt_ckpts/SpeechCLIP/base/flickr/parallel/epoch_131-step_15443-val_recall_mean_1_36.0100.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating: 100%|█████████▉| 624/625 [00:26<00:00, 28.79it/s]Total #1000 images, #5000 audio\n",
      "val_recall_AI {'recall@1': 26.739999651908875, 'recall@5': 56.940001249313354, 'recall@10': 69.84000205993652}\n",
      "val_recall_IA {'recall@1': 41.4000004529953, 'recall@5': 73.60000014305115, 'recall@10': 83.89999866485596}\n",
      "val_recall_mean {'recall@1': 34.07000005245209, 'recall@5': 65.27000069618225, 'recall@10': 76.87000036239624}\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 VALIDATE RESULTS\n",
      "{'val_cl_temp': 14.285706520080566,\n",
      " 'val_cl_temp_epoch': 14.285706520080566,\n",
      " 'val_loss': 0.1821601241827011,\n",
      " 'val_loss_epoch': 0.1821601241827011,\n",
      " 'val_p_cl_loss': 0.1821601241827011,\n",
      " 'val_p_cl_loss_epoch': 0.1821601241827011}\n",
      "--------------------------------------------------------------------------------\n",
      "Validating: 100%|██████████| 625/625 [00:26<00:00, 23.32it/s]\n"
     ]
    }
   ],
   "source": [
    "runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
