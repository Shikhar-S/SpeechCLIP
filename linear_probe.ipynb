{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (2000, 6) Test size: (1000, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# SpeechCLIP\n",
    "base_path = \"/data/user_data/sbharad2/SpeechCLIP/data\"\n",
    "embedding_read_path_pattern = (\n",
    "    \"{base_path}/Flickr8k.{csv_name}.token.txt.audio_embeddings/\"\n",
    ")\n",
    "\n",
    "# FLICKR\n",
    "csv_path_pattern = \"{base_path}/flickr/{csv_name}.csv\"\n",
    "\n",
    "train_df = pd.read_csv(\n",
    "    csv_path_pattern.format(base_path=base_path, csv_name=\"flickr_train_sampled\")\n",
    ")\n",
    "dev_df = pd.read_csv(\n",
    "    csv_path_pattern.format(base_path=base_path, csv_name=\"flickr_dev_sampled\")\n",
    ")\n",
    "\n",
    "# change target type as list\n",
    "train_df[\"target\"] = train_df[\"target\"].apply(eval)\n",
    "dev_df[\"target\"] = dev_df[\"target\"].apply(eval)\n",
    "\n",
    "print(\"Train size:\", train_df.shape, \"Test size:\", dev_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found for 1155138244_859fd6e079.jpg#0.npy\n",
      "File not found for 1468103286_96a6e07029.jpg#0.npy\n",
      "File not found for 1479857177_9d4a6f38fd.jpg#0.npy\n",
      "File not found for 1643915227_9f48068772.jpg#0.npy\n",
      "File not found for 1797554350_20998753c0.jpg#1.npy\n",
      "File not found for 1808504612_3508f3c9bb.jpg#0.npy\n",
      "File not found for 199463720_329a802206.jpg#0.npy\n",
      "File not found for 2058091220_2087270068.jpg#0.npy\n",
      "File not found for 2087317114_cf06df5aa5.jpg#1.npy\n",
      "File not found for 2136455112_202c093ba4.jpg#0.npy\n",
      "File not found for 2221818690_9003756d33.jpg#1.npy\n",
      "File not found for 2258277193_586949ec62.jpg.1#2.npy\n",
      "File not found for 2319197581_94f807b204.jpg#0.npy\n",
      "File not found for 236095031_5cb17dc54a.jpg#0.npy\n",
      "File not found for 2394824046_51cec8e5e7.jpg#1.npy\n",
      "File not found for 240696675_7d05193aa0.jpg#0.npy\n",
      "File not found for 2410153942_ba4a136358.jpg#0.npy\n",
      "File not found for 2428275562_4bde2bc5ea.jpg#1.npy\n",
      "File not found for 2553619107_d382a820f9.jpg#1.npy\n",
      "File not found for 2557972410_6925fe695c.jpg#2.npy\n",
      "File not found for 2582390123_71120edb0c.jpg#1.npy\n",
      "File not found for 2616508003_fa5ca5780d.jpg#0.npy\n",
      "Loaded (1978, 512) embeddings and (1978, 224) targets.\n",
      "Training shapes\n",
      "(1978, 512) (1978, 224)\n",
      "224 Target vocabulary size.\n",
      "Loaded (1000, 512) embeddings and (1000, 225) targets.\n",
      "Dev shapes\n",
      "(1000, 512) (1000, 225)\n",
      "225 Target vocabulary size.\n",
      "Original dev and train target shapes (1000, 225) (1978, 224)\n",
      "Removing columns from dev target\n",
      "New dev and train target shapes (1000, 224) (1978, 224)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "target_vocabulary = {}\n",
    "\n",
    "\n",
    "def _load_data(df, embedding_read_path, target_vocabulary=None):\n",
    "    all_embeddings = []\n",
    "    all_targets = []\n",
    "    for i, r in df.iterrows():\n",
    "        try:\n",
    "            example_id = r[\"example_id\"]\n",
    "            embedding = np.load(os.path.join(embedding_read_path, example_id) + \".npy\")\n",
    "            all_embeddings.append(embedding)\n",
    "            for tgt in r[\"target\"]:\n",
    "                if tgt not in target_vocabulary:\n",
    "                    target_vocabulary[tgt] = len(target_vocabulary)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found for {example_id}.npy\")\n",
    "\n",
    "    for i, r in df.iterrows():\n",
    "        example_id = r[\"example_id\"]\n",
    "        if not os.path.exists(os.path.join(embedding_read_path, example_id) + \".npy\"):\n",
    "            continue\n",
    "        target = np.zeros(len(target_vocabulary))\n",
    "        for tgt in r[\"target\"]:\n",
    "            target[target_vocabulary[tgt]] = 1\n",
    "        all_targets.append(target)\n",
    "\n",
    "    all_embeddings = np.array(all_embeddings)\n",
    "    all_targets = np.array(all_targets)\n",
    "    print(f\"Loaded {all_embeddings.shape} embeddings and {all_targets.shape} targets.\")\n",
    "    return all_embeddings, all_targets, target_vocabulary\n",
    "\n",
    "\n",
    "X_train, y_train, target_vocabulary = _load_data(\n",
    "    train_df,\n",
    "    embedding_read_path_pattern.format(\n",
    "        base_path=base_path, csv_name=\"flickr_train_sampled\"\n",
    "    ),\n",
    "    target_vocabulary,\n",
    ")\n",
    "print(\"Training shapes\")\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(len(target_vocabulary), \"Target vocabulary size.\")\n",
    "\n",
    "(\n",
    "    X_dev,\n",
    "    y_dev,\n",
    "    target_vocabulary,\n",
    ") = _load_data(\n",
    "    dev_df,\n",
    "    embedding_read_path_pattern.format(\n",
    "        base_path=base_path, csv_name=\"flickr_dev_sampled\"\n",
    "    ),\n",
    "    target_vocabulary,\n",
    ")\n",
    "print(\"Dev shapes\")\n",
    "print(X_dev.shape, y_dev.shape)\n",
    "print(len(target_vocabulary), \"Target vocabulary size.\")\n",
    "\n",
    "# Remove columns from y_dev if they are not present in y_train\n",
    "print(\"Original dev and train target shapes\", y_dev.shape, y_train.shape)\n",
    "if y_dev.shape[1] > y_train.shape[1]:\n",
    "    print(\"Removing columns from dev target\")\n",
    "    y_dev = y_dev[:, : y_train.shape[1]]\n",
    "print(\"New dev and train target shapes\", y_dev.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match: 15.6\n",
      "Micro averaged metrics\n",
      "F1 Score: 44.66257668711657\n",
      "Precision: 88.3495145631068\n",
      "Recall: 29.88505747126437\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "clf = MultiOutputClassifier(LogisticRegression()).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_dev)\n",
    "# Compute multi label metrics\n",
    "print(\"Exact Match:\", accuracy_score(y_dev, y_pred) * 100)\n",
    "\n",
    "print(\"Micro averaged metrics\")\n",
    "print(\"F1 Score:\", f1_score(y_dev, y_pred, average=\"micro\") * 100)\n",
    "print(\"Precision:\", precision_score(y_dev, y_pred, average=\"micro\") * 100)\n",
    "print(\"Recall:\", recall_score(y_dev, y_pred, average=\"micro\") * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
