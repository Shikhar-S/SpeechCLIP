{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Image embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "train_image_names_path = (\n",
    "    \"/data/user_data/sbharad2/SpeechCLIP/data/flickr/Flickr_8k.trainImages.txt\"\n",
    ")\n",
    "with open(train_image_names_path) as f:\n",
    "    train_image_names = f.readlines()\n",
    "train_image_names = [x.strip() for x in train_image_names]\n",
    "\n",
    "clip_model = \"ViT-B/32\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(clip_model, device=device)\n",
    "\n",
    "all_image_features = None\n",
    "c = 0\n",
    "with torch.no_grad():\n",
    "    for image_name in train_image_names:\n",
    "        image = (\n",
    "            preprocess(\n",
    "                Image.open(\n",
    "                    f\"/data/user_data/sbharad2/SpeechCLIP/data/flickr/Images/{image_name}\"\n",
    "                )\n",
    "            )\n",
    "            .unsqueeze(0)\n",
    "            .to(device)\n",
    "        )\n",
    "        image_features = model.encode_image(image)\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        if all_image_features is None:\n",
    "            all_image_features = image_features\n",
    "        else:\n",
    "            all_image_features += image_features\n",
    "        c += 1\n",
    "        if c % 100 == 0:\n",
    "            print(c)\n",
    "\n",
    "all_image_features = all_image_features / c\n",
    "all_image_features = all_image_features / all_image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "save_path = f\"/data/user_data/sbharad2/SpeechCLIP/data/flickr_stats/{clip_model.replace('/','_').replace('-','_')}.npy\"\n",
    "np.save(save_path, all_image_features.squeeze(0).cpu().numpy())\n",
    "print((all_image_features**2).sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "train_image_names_path = (\n",
    "    \"/data/user_data/sbharad2/SpeechCLIP/data/flickr/Flickr_8k.trainImages.txt\"\n",
    ")\n",
    "all_text_paths = \"/data/user_data/sbharad2/SpeechCLIP/data/flickr/Flickr8k.token.txt\"  # Make sure to filter out test set from this.\n",
    "\n",
    "with open(train_image_names_path) as f:\n",
    "    train_image_names = f.readlines()\n",
    "train_image_names = set([x.strip() for x in train_image_names])\n",
    "\n",
    "texts = []\n",
    "with open(all_text_paths) as f:\n",
    "    for line in f:\n",
    "        img_id, text = line.strip().split(\"\\t\")\n",
    "        img_id = img_id.split(\"#\")[0]\n",
    "        if img_id in train_image_names:\n",
    "            texts.append(text.strip())\n",
    "assert len(texts) == (len(train_image_names) * 5), (\n",
    "    len(texts),\n",
    "    len(train_image_names),\n",
    "    \"Mismatch in number of texts and images\",\n",
    ")\n",
    "print(len(texts), \"texts loaded\")\n",
    "\n",
    "clip_model = \"ViT-B/32\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(clip_model, device=device)\n",
    "\n",
    "all_text_features = None\n",
    "c = 0\n",
    "with torch.no_grad():\n",
    "    for text in texts:\n",
    "        text = clip.tokenize([text]).to(device)\n",
    "        text_features = model.encode_text(text)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        if all_text_features is None:\n",
    "            all_text_features = text_features\n",
    "        else:\n",
    "            all_text_features += text_features\n",
    "        c += 1\n",
    "        if c % 100 == 0:\n",
    "            print(c)\n",
    "\n",
    "all_text_features = all_text_features / c\n",
    "all_text_features = all_text_features / all_text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "save_path = f\"/data/user_data/sbharad2/SpeechCLIP/data/flickr_stats/text_stats.CLIP_{clip_model.replace('/','_').replace('-','_')}.npy\"\n",
    "np.save(save_path, all_text_features.squeeze(0).cpu().numpy())\n",
    "print((all_text_features**2).sum(dim=-1), c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whispclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
